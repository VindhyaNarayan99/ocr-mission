# -*- coding: utf-8 -*-
"""data140FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1edBYQRvDsmNgHS2QN7-uAgzu8jUCt-_y
"""

!ls /content/

!ls '/content/drive/My Drive/'

# Download a file from Colab to local machine
from google.colab import files
files.download('/content/your_file.txt')

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#!git clone https://github.com/ultralytics/yolov5  # clone
# %cd /content/drive/MyDrive/yolov5

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qr requirements.txt comet_ml  # install

import torch
import utils
display = utils.notebook_init()  # checks

import zipfile
import os

# Path to the zip file in Google Drive
zip_path = '/content/drive/MyDrive/Colab Notebooks/train_data1.zip' #/content/drive/MyDrive/Colab Notebooks/train_data.zip

# Directory to extract the contents to
extract_path = '/content/drive/MyDrive/Colab Notebooks/train_data1'

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List the contents of the extracted directory
print("Contents of the extracted directory:")
os.listdir(extract_path)

# Train YOLOv5s on COCO128 for 3 epochs
#!python train.py --img 640 --batch 2 --epochs 310 --data custom_data1.yaml --weights yolov5s.pt --cache disk --cache ram --nosave
!python train.py --resume /content/drive/MyDrive/yolov5/runs/train/exp4/weights/last.pt

#!python detect.py --weights runs/train/exp2/weights/last.pt --img 640 --conf 0.25 --source ../test_04.jpg
# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)
!python detect.py --weights runs/train/exp4/weights/last.pt --img 640 --conf 0.25 --source ../test_04.png --save-txt

!pip install pytesseract
!sudo apt install tesseract-ocr
!apt-get install tesseract-ocr-kan
!pip install ipdb
!pip install --upgrade Pillow
from PIL import Image
import pytesseract
import pdb
# Path to the Tesseract executable
pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'

# Mount Google Drive (if you are working with images stored in Google Drive)
from google.colab import drive
drive.mount('/content/drive')

#Weather image is read successfully
import cv2
import matplotlib.pyplot as plt

image_path = '/content/drive/MyDrive/test_04.png'

# Read the image using OpenCV
image = cv2.imread(image_path)
if image is None:
    print("Error: Unable to read the image using OpenCV.")
else:
    print("Image read successfully using OpenCV.")
    # Convert BGR to RGB for displaying correctly with matplotlib
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    plt.imshow(image_rgb)
    plt.title("Loaded Image")
    plt.show()

# Convert the image to PNG
converted_image_path = '/content/drive/MyDrive/test_04.png'
cv2.imwrite(converted_image_path, image)

# Try to open the converted image with PIL
from PIL import Image

try:
    converted_image = Image.open(converted_image_path)
    print("Converted image opened successfully with PIL.")
    # Perform OCR on the converted image
    content = pytesseract.image_to_string(converted_image, lang='kan')
    print("OCR Content:")
    print(content)
except Exception as e:
    print(f"Error in OCR extraction with converted image: {e}")

# Define the path to the annotations text file
annotations_file = '/content/drive/MyDrive/yolov5/runs/detect/exp80/labels/test_04.txt'

# Initialize an empty list to store annotations
annotations = []

# Read the annotations from the text file
with open(annotations_file, 'r') as file:
    # Iterate through each line in the file
    for line in file:
        # Split the line and append it to the annotations list
        annotations.append(line.strip())

# Print the annotations list to verify
print(annotations)

#CONTENT PORTION

from PIL import Image
import pytesseract
import cv2
import os
import numpy as np

def enhance_image(image):
    # Convert to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Apply Gaussian blur to reduce noise
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    # Apply adaptive thresholding to handle varying lighting conditions
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

def ocr_extract(image_path, class_idx, x_center_norm, y_center_norm, width_norm, height_norm):
    try:
        print("Image path:", image_path)
        image = Image.open(image_path)

        # Calculate actual coordinates based on image dimensions
        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        # Calculate ROI coordinates
        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        print(roi_x1)
        print(roi_y1)
        print(roi_x2)
        print(roi_y2)

        # Crop the image to extract the ROI
        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))

        # Enhance the cropped ROI image
        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)

        # Convert enhanced ROI back to PIL image
        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        # Perform OCR on the enhanced ROI
        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')

        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.png')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
#annotations = [
    #"0 0.5 0.5 0.5 0.5",  # Example annotation
#]

# Iterate through each annotation
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    print(class_idx)
    print(x_center_norm)
    print(y_center_norm)
    print(width_norm)
    print(height_norm)

    if class_idx == 1:
        image_path = converted_image_path
        print("Image path:", image_path)
        if os.path.exists(image_path):
            article_content = ocr_extract(image_path, class_idx, x_center_norm, y_center_norm, width_norm, height_norm)
            if article_content:
                print("Article Content:")
                print(article_content)
        else:
            print("Image file not found.")

# Install necessary packages
!apt-get update
!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-kan ffmpeg
!pip install gTTS pydub

from PIL import Image
import pytesseract
import cv2
import os
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display
import tempfile

def enhance_image(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

def ocr_extract(image_path, x_center_norm, y_center_norm, width_norm, height_norm):
    try:
        print("Image path:", image_path)
        image = Image.open(image_path)

        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        print(roi_x1)
        print(roi_y1)
        print(roi_x2)
        print(roi_y2)

        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))

        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)

        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')

        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None

def text_to_speech(text, lang='kn'):
    try:
        tts = gTTS(text, lang=lang)
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
            tts.save(f.name)
            audio_path = f.name
        display(Audio(audio_path, autoplay=True))
    except Exception as e:
        print(f"Error in TTS conversion: {e}")

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.png')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
annotations = [
    # Example annotations (class_idx, x_center_norm, y_center_norm, width_norm, height_norm)
    "0 0.5 0.5 0.5 0.5",
    "1 0.6185 0.154921 0.243 0.153098",
    "2 0.117 0.352977 0.158 0.18712"
]

# Extracting regions
headings = []
articles = []
contents = []

for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)

    if class_idx == 0:
        articles.append(bbox)
    elif class_idx == 1:
        headings.append(bbox)
    elif class_idx == 2:
        contents.append(bbox)

# Iterate through headings and find the corresponding articles and contents
for heading_bbox in headings:
    heading_text = ocr_extract(converted_image_path, *heading_bbox)
    if heading_text:
        print("Heading:", heading_text)
        text_to_speech(heading_text, lang='kn')
        user_input = input("Do you want to read the content for this heading? (yes/no): ")
        if user_input.lower() == 'yes':
            # Find the corresponding article
            for article_bbox in articles:
                if (heading_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                    heading_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                    heading_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                    heading_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                    print("Reading content for the article...")
                    for content_bbox in contents:
                        if (content_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                            content_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                            content_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                            content_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                            content_text = ocr_extract(converted_image_path, *content_bbox)
                            if content_text:
                                print("Content:", content_text)
                                text_to_speech(content_text, lang='kn')

from PIL import Image
import pytesseract
import cv2
import os
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display
import tempfile
import ipywidgets as widgets
from IPython.display import display, clear_output

def enhance_image(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

def ocr_extract(image_path, bbox):
    try:
        image = Image.open(image_path)
        x_center_norm, y_center_norm, width_norm, height_norm = bbox

        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))
        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)
        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')
        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None

def text_to_speech(text, lang='kn'):
    try:
        tts = gTTS(text, lang=lang)
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
            tts.save(f.name)
            audio_path = f.name
        display(Audio(audio_path, autoplay=True))
    except Exception as e:
        print(f"Error in TTS conversion: {e}")

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.jpg')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
# Format: "class_idx x_center_norm y_center_norm width_norm height_norm"
#annotations = [
    # Example annotations (class_idx, x_center_norm, y_center_norm, width_norm, height_norm)
   # "0 0.5 0.5 0.5 0.5",
   # "1 0.6185 0.154921 0.243 0.153098",
   # "2 0.117 0.352977 0.158 0.18712"
#]

# Extracting regions
headings = []
articles = []
contents = []

# Use model_3 to identify articles
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 0:
        articles.append(bbox)

# Use model_2 to identify headings and contents
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 1:
        headings.append(bbox)
    elif class_idx == 2:
        contents.append(bbox)

# Function to handle user interaction for each heading
def handle_heading(heading_text, heading_bbox):
    text_to_speech(heading_text, lang='kn')
    print("Heading:", heading_text)

    button_yes = widgets.Button(description="Yes")
    button_no = widgets.Button(description="No")
    output = widgets.Output()

    def on_yes_clicked(b):
        with output:
            clear_output()
            # Find the corresponding article
            for article_bbox in articles:
                if (heading_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                    heading_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                    heading_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                    heading_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                    print("Reading content for the article...")
                    for content_bbox in contents:
                        if (content_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                            content_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                            content_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                            content_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                            content_text = ocr_extract(converted_image_path, content_bbox)
                            if content_text:
                                print("Content:", content_text)
                                text_to_speech(content_text, lang='kn')
                    break
        display(next_heading())

    def on_no_clicked(b):
        with output:
            clear_output()
        display(next_heading())

    button_yes.on_click(on_yes_clicked)
    button_no.on_click(on_no_clicked)

    display(widgets.HBox([button_yes, button_no]), output)

# Function to get the next heading
def next_heading():
    if headings:
        heading_bbox = headings.pop(0)
        heading_text = ocr_extract(converted_image_path, heading_bbox)
        if heading_text:
            return handle_heading(heading_text, heading_bbox)
    else:
        print("No more headings.")
        return None

# Start the process
next_heading()

from PIL import Image
import pytesseract
import cv2
import os
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display
import tempfile
import ipywidgets as widgets
from IPython.display import display, clear_output

def enhance_image(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

def ocr_extract(image_path, bbox):
    try:
        image = Image.open(image_path)
        x_center_norm, y_center_norm, width_norm, height_norm = bbox

        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))
        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)
        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')
        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None


def text_to_speech(text, lang='kn'):
    try:
        tts = gTTS(text, lang=lang)
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
            tts.save(f.name)
            audio_path = f.name
        display(Audio(audio_path, autoplay=True))
    except Exception as e:
        print(f"Error in TTS conversion: {e}")

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.jpg')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
# Format: "class_idx x_center_norm y_center_norm width_norm height_norm"
#annotations = [
    # Example annotations (class_idx, x_center_norm, y_center_norm, width_norm, height_norm)
    #"0 0.5 0.5 0.5 0.5",
    #"1 0.6185 0.154921 0.243 0.153098",
    #"2 0.117 0.352977 0.158 0.18712"
#]

# Extracting regions
headings = []
articles = []
contents = []

# Use model_3 to identify articles
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 0:
        articles.append(bbox)

# Use model_2 to identify headings and contents
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 1:
        headings.append(bbox)
    elif class_idx == 2:
        contents.append(bbox)

# State variable to keep track of whether we are reading headings or contents
state = 'reading_headings'


# Function to handle user interaction for each heading
def handle_heading(heading_text, heading_bbox):
    global state
    state = 'reading_headings'
    text_to_speech(heading_text, lang='kn')
    print("Heading:", heading_text)

    button_yes = widgets.Button(description="Yes")
    button_no = widgets.Button(description="No")
    output = widgets.Output()

    def on_yes_clicked(b):
        global state
        if state == 'reading_headings':
            state = 'reading_contents'
            with output:
                clear_output()
                read_contents(heading_bbox)

    def on_no_clicked(b):
        global state
        if state == 'reading_headings':
            with output:
                clear_output()
            display(next_heading())

    button_yes.on_click(on_yes_clicked)
    button_no.on_click(on_no_clicked)

    display(widgets.HBox([button_yes, button_no]), output)

# Function to read contents associated with a heading
def read_contents(heading_bbox):
    global state
    if state == 'reading_contents':
        for article_bbox in articles:
            if (heading_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                heading_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                heading_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                heading_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                print("Reading content for the article...")
                for content_bbox in contents:
                    if (content_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                        content_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                        content_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                        content_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                        content_text = ocr_extract(converted_image_path, content_bbox)
                        if content_text:
                            print("Content:", content_text)
                            text_to_speech(content_text, lang='kn')
                break

        state = 'waiting_for_next_heading'
        display(wait_for_next_heading())

def wait_for_next_heading():
    button_next = widgets.Button(description="Next Heading")
    output = widgets.Output()

    def on_next_clicked(b):
        global state
        if state == 'waiting_for_next_heading':
            with output:
                clear_output()
            state = 'reading_headings'
            display(next_heading())

    button_next.on_click(on_next_clicked)
    display(button_next, output)

# Function to get the next heading
def next_heading():
    global state
    if state == 'reading_headings':
        if headings:
            heading_bbox = headings.pop(0)
            print(f"Processing heading with bbox: {heading_bbox}")
            heading_text = ocr_extract(converted_image_path, heading_bbox)
            if heading_text:
                print(f"Extracted heading text: {heading_text}")
                return handle_heading(heading_text, heading_bbox)
            else:
                print("No text extracted from heading.")
                return next_heading()
        else:
            print("No more headings.")
            return None

# Start the process
next_heading()

from PIL import Image
import pytesseract
import cv2
import os
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display, clear_output
import tempfile
import ipywidgets as widgets

# Function to enhance image for better OCR results
def enhance_image(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

# Function to perform OCR on a specific region defined by bbox
def ocr_extract(image_path, bbox):
    try:
        image = Image.open(image_path)
        x_center_norm, y_center_norm, width_norm, height_norm = bbox

        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))
        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)
        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')
        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None

# Function to convert text to speech
def text_to_speech(text, lang='kn'):
    try:
        tts = gTTS(text, lang=lang)
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
            tts.save(f.name)
            audio_path = f.name
        return Audio(audio_path, autoplay=True)
    except Exception as e:
        print(f"Error in TTS conversion: {e}")
        return None

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.jpg')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
#annotations = [
    #"0 0.5 0.5 0.5 0.5",
    #"1 0.6185 0.154921 0.243 0.153098",
    #"2 0.117 0.352977 0.158 0.18712"
#]

# Extracting regions
headings = []
articles = []
contents = []

# Use model_3 to identify articles
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 0:
        articles.append(bbox)

# Use model_2 to identify headings and contents
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 1:
        headings.append(bbox)
    elif class_idx == 2:
        contents.append(bbox)

# State variable to keep track of whether we are reading headings or contents
state = 'reading_headings'
current_audio = None  # Variable to hold the current audio playback

# Function to handle user interaction for each heading
def handle_heading(heading_text, heading_bbox):
    global state, current_audio
    state = 'reading_headings'

    # Stop current audio if it is playing
    if current_audio:
        current_audio.stop()

    # Convert heading text to speech
    audio = text_to_speech(heading_text, lang='kn')
    if audio:
        current_audio = audio

    print("Heading:", heading_text)

    button_yes = widgets.Button(description="Yes")
    button_no = widgets.Button(description="No")
    output = widgets.Output()

    def on_yes_clicked(b):
        global state, current_audio
        if state == 'reading_headings':
            state = 'reading_contents'
            with output:
                clear_output()
                read_contents(heading_bbox)

    def on_no_clicked(b):
        global state
        if state == 'reading_headings':
            with output:
                clear_output()
            display(next_heading())

    button_yes.on_click(on_yes_clicked)
    button_no.on_click(on_no_clicked)

    display(widgets.HBox([button_yes, button_no]), output)

# Function to read contents associated with a heading
def read_contents(heading_bbox):
    global state, current_audio
    if state == 'reading_contents':
        # Stop current audio if it is playing
        if current_audio:
            display(current_audio)  # Display current audio if it's still playing
            current_audio = None  # Clear current audio
            clear_output(wait=True)

        for article_bbox in articles:
            if (heading_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                heading_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                heading_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                heading_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                print("Reading content for the article...")
                for content_bbox in contents:
                    if (content_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                        content_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                        content_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                        content_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                        content_text = ocr_extract(converted_image_path, content_bbox)
                        if content_text:
                            print("Content:", content_text)
                            audio = text_to_speech(content_text, lang='kn')
                            if audio:
                                current_audio = audio
                                display(current_audio)
                break

        state = 'waiting_for_next_heading'
        display(wait_for_next_heading())

# Function to display "Next Heading" button
def wait_for_next_heading():
    button_next = widgets.Button(description="Next Heading")
    output = widgets.Output()

    def on_next_clicked(b):
        global state, current_audio
        if state == 'waiting_for_next_heading':
            with output:
                clear_output()
            state = 'reading_headings'
            display(next_heading())

    button_next.on_click(on_next_clicked)
    display(button_next, output)

# Function to get the next heading
def next_heading():
    global state, current_audio
    if state == 'reading_headings':
        # Stop current audio if it is playing
        if current_audio:
            display(current_audio)  # Display current audio if it's still playing
            current_audio = None  # Clear current audio
            clear_output(wait=True)

        if headings:
            heading_bbox = headings.pop(0)
            print(f"Processing heading with bbox: {heading_bbox}")
            heading_text = ocr_extract(converted_image_path, heading_bbox)
            if heading_text:
                print(f"Extracted heading text: {heading_text}")
                return handle_heading(heading_text, heading_bbox)
            else:
                print("No text extracted from heading.")
                return next_heading()
        else:
            print("No more headings.")
            return None

# Start the process
next_heading()

#CORRECT CODE
from PIL import Image
import pytesseract
import cv2
import os
import numpy as np
from gtts import gTTS
from IPython.display import Audio, display, clear_output
import tempfile
import ipywidgets as widgets

# Function to enhance image for better OCR results
def enhance_image(image):
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
    enhanced_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY, 11, 2)
    return enhanced_image

# Function to perform OCR on a specific region defined by bbox
def ocr_extract(image_path, bbox):
    try:
        image = Image.open(image_path)
        x_center_norm, y_center_norm, width_norm, height_norm = bbox

        x_center = x_center_norm * image.width
        y_center = y_center_norm * image.height
        roi_width = width_norm * image.width
        roi_height = height_norm * image.height

        roi_x1 = int(max(0, x_center - roi_width / 2))
        roi_y1 = int(max(0, y_center - roi_height / 2))
        roi_x2 = int(min(image.width, x_center + roi_width / 2))
        roi_y2 = int(min(image.height, y_center + roi_height / 2))

        roi = image.crop((roi_x1, roi_y1, roi_x2, roi_y2))
        roi_cv = cv2.cvtColor(np.array(roi), cv2.COLOR_RGB2BGR)
        enhanced_roi = enhance_image(roi_cv)
        enhanced_roi_pil = Image.fromarray(enhanced_roi)

        content = pytesseract.image_to_string(enhanced_roi_pil, lang='kan', config='--psm 6')
        return content
    except Exception as e:
        print(f"Error in OCR extraction: {e}")
        return None

# Function to convert text to speech
def text_to_speech(text, lang='kn'):
    try:
        tts = gTTS(text, lang=lang)
        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as f:
            tts.save(f.name)
            audio_path = f.name
        return Audio(audio_path, autoplay=True)
    except Exception as e:
        print(f"Error in TTS conversion: {e}")
        return None

# Define the base path for Google Drive in Colab
base_path = '/content/drive/MyDrive/'

# Convert the image to PNG using OpenCV
original_image_path = os.path.join(base_path, 'test_04.jpg')
converted_image_path = os.path.join(base_path, 'test_04_converted.png')

# Read and convert the image
image = cv2.imread(original_image_path)
if image is not None:
    cv2.imwrite(converted_image_path, image)
    print("Image converted to PNG format.")
else:
    print("Error: Unable to read the original image using OpenCV.")

# Assuming you have loaded annotations from the text file into the 'annotations' list
#annotations = [
  #  "0 0.5 0.5 0.5 0.5",
   # "1 0.6185 0.154921 0.243 0.153098",
   # "2 0.117 0.352977 0.158 0.18712"
#]

# Extracting regions
headings = []
articles = []
contents = []

# Use model_3 to identify articles
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 0:
        articles.append(bbox)

# Use model_2 to identify headings and contents
for annotation in annotations:
    class_idx, x_center_norm, y_center_norm, width_norm, height_norm = map(float, annotation.split())
    bbox = (x_center_norm, y_center_norm, width_norm, height_norm)
    if class_idx == 1:
        headings.append(bbox)
    elif class_idx == 2:
        contents.append(bbox)

# State variable to keep track of whether we are reading headings or contents
state = 'reading_headings'
current_audio = None  # Variable to hold the current audio playback

# Function to handle user interaction for each heading
def handle_heading(heading_text, heading_bbox):
    global state, current_audio
    state = 'reading_headings'

    # Stop current audio if it is playing
    clear_output(wait=True)

    # Convert heading text to speech
    audio = text_to_speech(heading_text, lang='kn')
    if audio:
        current_audio = audio
        display(current_audio)

    print("Heading:", heading_text)

    button_yes = widgets.Button(description="Yes")
    button_no = widgets.Button(description="No")
    output = widgets.Output()

    def on_yes_clicked(b):
        global state, current_audio
        if state == 'reading_headings':
            state = 'reading_contents'
            clear_output(wait=True)
            read_contents(heading_bbox)

    def on_no_clicked(b):
        global state
        if state == 'reading_headings':
            clear_output(wait=True)
            display(next_heading())

    button_yes.on_click(on_yes_clicked)
    button_no.on_click(on_no_clicked)

    display(widgets.HBox([button_yes, button_no]), output)

# Function to read contents associated with a heading
def read_contents(heading_bbox):
    global state, current_audio
    if state == 'reading_contents':
        # Stop current audio if it is playing
        clear_output(wait=True)

        for article_bbox in articles:
            if (heading_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                heading_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                heading_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                heading_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                print("Reading content for the article...")
                for content_bbox in contents:
                    if (content_bbox[0] >= article_bbox[0] - article_bbox[2] / 2 and
                        content_bbox[0] <= article_bbox[0] + article_bbox[2] / 2 and
                        content_bbox[1] >= article_bbox[1] - article_bbox[3] / 2 and
                        content_bbox[1] <= article_bbox[1] + article_bbox[3] / 2):
                        content_text = ocr_extract(converted_image_path, content_bbox)
                        if content_text:
                            print("Content:", content_text)
                            audio = text_to_speech(content_text, lang='kn')
                            if audio:
                                current_audio = audio
                                display(current_audio)
                break

        state = 'waiting_for_next_heading'
        display(wait_for_next_heading())

# Function to display "Next Heading" button
def wait_for_next_heading():
    button_next = widgets.Button(description="Next Heading")
    output = widgets.Output()

    def on_next_clicked(b):
        global state, current_audio
        if state == 'waiting_for_next_heading':
            clear_output(wait=True)
            state = 'reading_headings'
            display(next_heading())

    button_next.on_click(on_next_clicked)
    display(button_next, output)

# Function to get the next heading
def next_heading():
    global state, current_audio
    if state == 'reading_headings':
        # Stop current audio if it is playing
        clear_output(wait=True)

        if headings:
            heading_bbox = headings.pop(0)
            print(f"Processing heading with bbox: {heading_bbox}")
            heading_text = ocr_extract(converted_image_path, heading_bbox)
            if heading_text:
                print(f"Extracted heading text: {heading_text}")
                handle_heading(heading_text, heading_bbox)
            else:
                print("No text extracted from heading.")
                next_heading()
        else:
            print("No more headings.")
            return None

# Start the process
next_heading()